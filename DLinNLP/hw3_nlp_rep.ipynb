{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Information about the submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3",
      "metadata": {
        "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3"
      },
      "source": [
        "## 1.1 Name and number of the assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9",
      "metadata": {
        "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9"
      },
      "source": [
        "#### #3 Text Detoxification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d",
      "metadata": {
        "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d"
      },
      "source": [
        "## 1.2 Student name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "Ivan Gurev"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a46ab45-d215-41af-b910-63ff4a215a07",
      "metadata": {
        "id": "8a46ab45-d215-41af-b910-63ff4a215a07"
      },
      "source": [
        "## 1.3 Codalab user ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a",
      "metadata": {
        "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a"
      },
      "source": [
        "IvanGurev"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70456c74-8e1f-4da0-bebe-fbceee169115",
      "metadata": {
        "id": "70456c74-8e1f-4da0-bebe-fbceee169115"
      },
      "source": [
        "## 1.4 Additional comments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea",
      "metadata": {
        "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea"
      },
      "source": [
        "***Enter here** any additional comments which you would like to communicate to a TA who is going to grade this work not related to the content of your submission.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af498ab-3c00-4d36-a962-c947862fede8",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Technical Report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3c18a6-868b-4357-a308-7f6dff05c3d0",
      "metadata": {
        "id": "9b3c18a6-868b-4357-a308-7f6dff05c3d0"
      },
      "source": [
        "*Use Section 2 to describe results of your experiments as you would do writing a paper about your results. DO NOT insert code in this part. Only insert plots and tables summarizing results as needed. Use formulas if needed do described your methodology. The code is provided in Section 3.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061f71b9-114a-4cb0-b531-5711970317bf",
      "metadata": {
        "id": "061f71b9-114a-4cb0-b531-5711970317bf"
      },
      "source": [
        "## 2.1 Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3",
      "metadata": {
        "id": "c327f43e-ed30-4279-bba2-a97b2f8ef9e3"
      },
      "source": [
        "In this task I was aimed to provide model which is able to detoxify input text.\n",
        "\n",
        "### Preprocessing\n",
        "The initial dataset is tsv file, which contains toxic comments `toxic_comment` and several neutral rephrases `neutral_comment1`,`neutral_comment2`,`neutral_comment3` for them . Since amount of rephrases for each toxic comment varies, the structure of the file was remade to `toxic_comment`, `neutral_comment`. After that standart pipeline for training (`Dataset`, `Data_collator`) was done.\n",
        "\n",
        "\n",
        "### Modelling\n",
        "\n",
        "As a detoxificator `T5ForConditionalGeneration` ru_base was used. Model was pretrained by **ai-forever**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe27e49-10c7-4c12-adea-48b0a05a5681",
      "metadata": {
        "id": "afe27e49-10c7-4c12-adea-48b0a05a5681"
      },
      "source": [
        "## 2.2 Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b1c84c-c261-46b5-a009-0f2bc4002752",
      "metadata": {
        "id": "b5b1c84c-c261-46b5-a009-0f2bc4002752"
      },
      "source": [
        "After trainnig for 10 epochs model shows appropriate results breaking the baseline. I suppose results can be improved by adding text augmentations and training for more epochs.\n",
        "\n",
        "Method | STA |  MP | Fluency | Joint Score | ChrF1\n",
        "--- | --- | --- | --- | --- | ---\n",
        "Baseline | 0.56 | 0.89 | 0.85 | 0.41 | 0.53\n",
        "T5 on train set | 0.72 | 0.81 | 0.84| 0.48 | 0.56\n",
        "T5 on test set  | 0.75 | 0.83 | 0.81 | 0.51 | 0.57\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1",
      "metadata": {
        "id": "a33ff9bd-62c6-4a63-8600-b1651420fee1"
      },
      "source": [
        "*Enter here all code used to produce your results submitted to Codalab. Add some comments and subsections to navigate though your solution.*\n",
        "\n",
        "*In this part you are expected to develop yourself a solution of the task and provide a reproducible code:*\n",
        "- *Using Python 3;*\n",
        "- *Contains code for installation of all dependencies;*\n",
        "- *Contains code for downloading of all the datasets used*;\n",
        "- *Contains the code for reproducing your results (in other words, if a tester downloads your notebook she should be able to run cell-by-cell the code and obtain your experimental results as described in the methodology section)*.\n",
        "\n",
        "\n",
        "*As a result, you code will be graded according to these criteria:*\n",
        "- ***Readability**: your code should be well-structured preferably with indicated parts of your approach (Preprocessing, Model training, Evaluation, etc.).*\n",
        "- ***Reproducibility**: your code should be reproduced without any mistakes with “Run all” mode (obtaining experimental part).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff93e37-3a24-40ab-87db-16b537aad3f6",
      "metadata": {
        "id": "dff93e37-3a24-40ab-87db-16b537aad3f6"
      },
      "source": [
        "## 3.1 Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73daa932-114b-4e28-9141-13b57c729435",
      "metadata": {
        "id": "73daa932-114b-4e28-9141-13b57c729435",
        "outputId": "bc3fc620-f4f0-40f8-d93e-470cc109fb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece sacrebleu\n",
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (T5ForConditionalGeneration,\n",
        "                          T5Tokenizer,\n",
        "                          Seq2SeqTrainer,\n",
        "                          Seq2SeqTrainingArguments,\n",
        "                      )\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "from transformers.file_utils import cached_property\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXduSn3EnJh0",
        "outputId": "5ac8d6b5-993a-4402-8fbb-1f1722304e00"
      },
      "id": "GXduSn3EnJh0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.2 Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed8aa6f0-79e0-4c7d-b6bc-2dcd48382af2",
      "metadata": {
        "id": "ed8aa6f0-79e0-4c7d-b6bc-2dcd48382af2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/train.tsv',sep='\t', header=0)\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/test.tsv',sep='\t', header=0)\n",
        "df.to_csv('drive/MyDrive/t5_detox/train.tsv', sep = '\\t')\n",
        "test.to_csv('drive/MyDrive/t5_detox/test.tsv', sep = '\\t')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791dc0e7-337d-46ad-96a3-543a732f19e2",
      "metadata": {
        "id": "791dc0e7-337d-46ad-96a3-543a732f19e2"
      },
      "source": [
        "## 3.3 Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71a4653-86f2-448a-a2c4-17c02c1ba940",
      "metadata": {
        "tags": [],
        "id": "b71a4653-86f2-448a-a2c4-17c02c1ba940"
      },
      "outputs": [],
      "source": [
        "df = df.fillna('')\n",
        "\n",
        "df_train_toxic = []\n",
        "df_train_neutral = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
        "\n",
        "    for reference in references:\n",
        "        if len(reference) > 0:\n",
        "            df_train_toxic.append(row['toxic_comment'])\n",
        "            df_train_neutral.append(reference)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'toxic_comment': df_train_toxic,\n",
        "    'neutral_comment': df_train_neutral\n",
        "})\n",
        "\n",
        "df = shuffle(df)\n",
        "\n",
        "\n",
        "class PairsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.x['input_ids'])\n",
        "        item = {key: val[idx] for key, val in self.x.items()}\n",
        "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
        "        item['labels'] = self.y['input_ids'][idx]\n",
        "        return item\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        return len(self.x['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n # * 2\n",
        "\n",
        "from typing import List, Dict, Union\n",
        "\n",
        "class DataCollatorWithPadding:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "        )\n",
        "        ybatch = self.tokenizer.pad(\n",
        "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
        "            padding=True,\n",
        "        )\n",
        "        batch['labels'] = ybatch['input_ids']\n",
        "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
        "\n",
        "        return {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a",
      "metadata": {
        "id": "23773d4e-e8d7-4e56-9610-4ee61b38c65a"
      },
      "source": [
        "## 3.4 My method of text processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4aa2274-477a-46e6-95ff-ec98e633ab35",
      "metadata": {
        "id": "f4aa2274-477a-46e6-95ff-ec98e633ab35"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.model_name_or_path = 'ai-forever/ruT5-base'\n",
        "        self.model_class = T5ForConditionalGeneration\n",
        "        self.tokenizer_class = T5Tokenizer\n",
        "\n",
        "\n",
        "class detoxT5:\n",
        "\n",
        "    def __init__(self, pretrained ='drive/MyDrive/t5_detox/t5detox.pt', device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        self.args = Args()\n",
        "        self.device = device\n",
        "\n",
        "        model_class, tokenizer_class = self.args.model_class, self.args.tokenizer_class\n",
        "        self.tokenizer = tokenizer_class.from_pretrained(self.args.model_name_or_path)\n",
        "        self.model = model_class.from_pretrained(self.args.model_name_or_path)\n",
        "        if pretrained:\n",
        "            self.model.load_state_dict(torch.load(pretrained))\n",
        "            print(f'loading pretrained model from {pretrained}')\n",
        "        self.model.to(device)\n",
        "\n",
        "\n",
        "    def train(self,x,y,test_size=0.15, epochs = 5):\n",
        "\n",
        "\n",
        "        x1, x2, y1, y2 = train_test_split(x, y, test_size=test_size, random_state=42)\n",
        "        train_dataset = PairsDataset(self.tokenizer(x1), self.tokenizer(y1))\n",
        "        test_dataset = PairsDataset(self.tokenizer(x2), self.tokenizer(y2))\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "\n",
        "\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "                                  output_dir=\"bart_detox\",\n",
        "                                  do_train=True,\n",
        "                                  do_eval=True,\n",
        "                                  per_device_train_batch_size=16,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  weight_decay=1e-5,\n",
        "                                  num_train_epochs=epochs,\n",
        "                                  learning_rate=3e-5,\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  save_strategy=\"no\",\n",
        "                                  save_total_limit=1,\n",
        "                                  logging_steps=300,\n",
        "                                  gradient_accumulation_steps=1,\n",
        "                              )\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "                                  model=self.model,\n",
        "                                  args=training_args,\n",
        "                                  data_collator = data_collator,\n",
        "                                  train_dataset=train_dataset,\n",
        "                                  eval_dataset=test_dataset,\n",
        "                                  tokenizer=self.tokenizer,\n",
        "                              )\n",
        "        trainer.train()\n",
        "\n",
        "    def paraphrase(self,text, n=None, max_length='auto', temperature=0.0, beams=3):\n",
        "        texts = [text] if isinstance(text, str) else text\n",
        "        inputs = self.tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(self.model.device)\n",
        "        if max_length == 'auto':\n",
        "            max_length = int(inputs.shape[1] * 1.2) + 10\n",
        "            result = self.model.generate(\n",
        "                                            inputs,\n",
        "                                            num_return_sequences=n or 1,\n",
        "                                            do_sample=False,\n",
        "                                            temperature=temperature,\n",
        "                                            repetition_penalty=3.0,\n",
        "                                            max_length=max_length,\n",
        "                                            bad_words_ids=[[2]],\n",
        "                                            num_beams=beams,\n",
        "                                        )\n",
        "        texts = [self.tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
        "        if not n and isinstance(text, str):\n",
        "            return texts[0]\n",
        "        return texts\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhVi_jp9DOpL"
   },
   "source": [
    "# MPI (Message-Passing Interface)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "* Mechanism of data exchange between processes\n",
    "* Two types of communication: \n",
    " * **point-to-point**: between two processes \n",
    " * **collective**: between multiple processes\n",
    "* Typically only one program, branching depending on the process \n",
    "* Using the mpi4py Python library \n",
    "\n",
    "An mpi4py tutorial: \n",
    "* https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
    "\n",
    "\n",
    "Install mpi4py:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgyeyISlEBPQ",
    "outputId": "25dc480d-cf58-44aa-c13c-047b7d8b6efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpi4py\n",
      "  Downloading mpi4py-3.1.3-cp310-cp310-win_amd64.whl (539 kB)\n",
      "     -------------------------------------- 539.9/539.9 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: mpi4py\n",
      "Successfully installed mpi4py-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89pnn-HgDpjV"
   },
   "source": [
    "## A basic example (no data exchange)\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wk1x92wcDOpR",
    "outputId": "24c7dd16-13e0-4ffb-ce85-e65e9f5890ab"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing MPI: Не найден указанный модуль.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mpi.py\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpi4py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPI\n\u001b[0;32m      3\u001b[0m comm \u001b[38;5;241m=\u001b[39m MPI\u001b[38;5;241m.\u001b[39mCOMM_WORLD\n\u001b[0;32m      4\u001b[0m rank \u001b[38;5;241m=\u001b[39m comm\u001b[38;5;241m.\u001b[39mGet_rank() \u001b[38;5;66;03m# index of the current process \u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing MPI: Не найден указанный модуль."
     ]
    }
   ],
   "source": [
    "# mpi.py\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank() # index of the current process \n",
    "print (\"hello from process \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgRysQhOEt3f",
    "outputId": "5ef41ef4-4ebd-4f22-9ac0-b9175bb52fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello from process  1\n",
      "hello from process  0\n",
      "hello from process  2\n"
     ]
    }
   ],
   "source": [
    "!mpirun #--allow-run-as-root# -n 3 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UptBtCQoDOqK"
   },
   "source": [
    "## Point-to-point communication of two processes\n",
    "\n",
    "### Example: computing $\\pi$ with MPI \n",
    "\n",
    "$$\\pi=\\sqrt{6\\sum_{n=1}^{\\infty}\\frac{1}{n^2}}$$\n",
    "\n",
    "**Exercise:** Theoretically estimate the error resulting if we truncate the series at $N$ terms.  \n",
    "\n",
    "Without MPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJeSQbxbDOqU",
    "outputId": "8a572fa0-4501-462b-d4d2-24a2d5c7deee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415878789259364\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(1,200000)\n",
    "print (np.sqrt(6*np.sum(1./(a*a))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrFHlcHIDOqz"
   },
   "source": [
    "### Functions ``send()``, ``recv()``\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 2 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NFhjMInDOq4"
   },
   "outputs": [],
   "source": [
    "# Evaluate the sum of 2M terms by splitting into two groups of M terms.\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank() \n",
    "\n",
    "M = 100\n",
    "def getPartialSum(start, end):\n",
    "    a = np.arange(start, end)\n",
    "    return np.sum(1./(a*a))\n",
    "\n",
    "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
    "print ('Process', rank, 'found partial sum from term', 1+rank*M, 'to term', 1+(rank+1)*M-1, ': ', s )\n",
    "\n",
    "# process 1 sends its partial sum to process 0\n",
    "if rank == 1:\n",
    "    comm.send(s, dest=0) \n",
    "    \n",
    "# process 0 receives the partial sum from process 1, adds to its own partial sum\n",
    "# and outputs the result    \n",
    "elif rank == 0: \n",
    "    s_other = comm.recv(source=1)\n",
    "    s_total = s+s_other\n",
    "    print ('total partial sum =', s_total)\n",
    "    print ('pi_approx =', np.sqrt(6*s_total))\n",
    "    \n",
    "print ('Process', rank, 'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyaN4fOCFnfN",
    "outputId": "3df66fc9-8da2-4bce-d96d-96a7a1120689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 found partial sum from term 1 to term 100 :  1.6349839001848931\n",
      "Process 1 found partial sum from term 101 to term 200 :  0.004962645830104402\n",
      "Process 1 finished\n",
      "total partial sum = 1.6399465460149976\n",
      "pi_approx = 3.1368263063309683\n",
      "Process 0 finished\n"
     ]
    }
   ],
   "source": [
    "!mpirun --allow-run-as-root -n 2 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-wSXWdXH1pY"
   },
   "source": [
    "**Exercise:** Perform the same computation in a \"ping-pong\" manner: one process sums only even terms, the other only odd terms; after adding a new term the process sends the current result to the other process.  \n",
    "\n",
    "## Collective communication\n",
    "\n",
    "Perform efficient (fast, load-balanced) collective operations (e.g., summations) involving multiple processes. \n",
    "\n",
    "<img src='https://materials.jeremybejarano.com/MPIwithPython/_images/fastSum.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lL5OC2CNDOrY"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename=\"fastSum.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5eg3eZdDOr1"
   },
   "source": [
    "### Function ``gather()``\n",
    "\n",
    "Pass data from all processes to the chosen process.\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 5 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGNWAOcPDOr-"
   },
   "outputs": [],
   "source": [
    "# Evaluate the sum of MN terms by splitting into M groups of N terms.\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size() # total number of processes\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "M = 100\n",
    "def getPartialSum(start, end):\n",
    "    a = np.arange(start, end)\n",
    "    return np.sum(1./(a*a))\n",
    "\n",
    "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
    "\n",
    "partialSums = comm.gather(s, root=0)\n",
    "print ('partialSums gathered at process %d:' %(rank), partialSums) \n",
    "\n",
    "if rank == 0:\n",
    "    print ('pi_approx =', np.sqrt(6*np.sum(partialSums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWUfMyhSGG6D",
    "outputId": "21a050c6-f779-4f2d-ff2c-9274c966191b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"mpirun\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!mpirun  -n 5 py mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCXH5iJXICPd"
   },
   "source": [
    "### Function ``bcast()``\n",
    "\n",
    "Pass data from the chosen process to all other processes.\n",
    "\n",
    "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mw2GT71eDOsY"
   },
   "outputs": [],
   "source": [
    "# basic usage of bcast()\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    some_data = {0: 'abcd', 1:1234}\n",
    "else:\n",
    "    some_data = None\n",
    "    \n",
    "print (\"I'm process\", rank, '; data before broadcasting:', some_data)\n",
    "data = comm.bcast(some_data, root=0)\n",
    "print (\"I'm process\", rank, '; data after broadcasting:', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRlOFLVFGL-5",
    "outputId": "b7248d33-9128-46b3-ea52-34409256e648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm process 0 ; data before broadcasting: {0: 'abcd', 1: 1234}\n",
      "I'm process 2 ; data before broadcasting: None\n",
      "I'm process 0 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
      "I'm process 2 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
      "I'm process 1 ; data before broadcasting: None\n",
      "I'm process 1 ; data after broadcasting: {0: 'abcd', 1: 1234}\n"
     ]
    }
   ],
   "source": [
    "!mpirun --allow-run-as-root -n 3 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNm6buMYITTW"
   },
   "source": [
    "### Functions ``scatter()``, ``reduce()``\n",
    "\n",
    "* ``scatter()``: distribute data from one source to all processes\n",
    "* ``reduce()``: combine data from all processes using a collective operation like `sum` or `max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTGFw1YLDOsq"
   },
   "outputs": [],
   "source": [
    "# Evaluate the sum of N terms by scattering them to N processes.\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data2scatter = [a*a for a in range(1,size+1)]\n",
    "else:\n",
    "    data2scatter = None\n",
    "    \n",
    "data = comm.scatter(data2scatter, root=0)\n",
    "\n",
    "print ('Data at process', rank, ':', data)\n",
    "\n",
    "b = 1./data\n",
    "\n",
    "partialSum = comm.reduce(b, op = MPI.SUM, root = 0)\n",
    "\n",
    "print ('Partial sum at process', rank, ':', partialSum)\n",
    "\n",
    "if rank == 0:\n",
    "    result = np.sqrt(6*partialSum)\n",
    "    print ('Pi_approx:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5NWqw3HGRTX",
    "outputId": "65f75fda-24ba-4319-9a8b-deffff478aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at process 0 : 1\n",
      "Data at process 2 : 9\n",
      "Data at process 1 : 4\n",
      "Data at process 3 : 16\n",
      "Data at process 4 : 25\n",
      "Partial sum at process 3 : None\n",
      "Partial sum at process 2 : None\n",
      "Partial sum at process 1 : None\n",
      "Partial sum at process 4 : None\n",
      "Partial sum at process 0 : 1.4636111111111112\n",
      "Pi_approx: 2.9633877010385707\n"
     ]
    }
   ],
   "source": [
    "!mpirun --allow-run-as-root -n 5 python mpi.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Tmm87fhIdwU"
   },
   "source": [
    "## Example: parallel scalar product\n",
    "* Generate two random vectors $\\mathbf x$ and $\\mathbf y$ at the root process. Goal: compute their scalar product $\\langle\\mathbf x,\\mathbf y\\rangle$ \n",
    "* Divide $\\mathbf x$ and $\\mathbf y$ into chunks and scatter them to the other processes\n",
    "* Compute scalar products between chunks at each process\n",
    "* Obtain $\\langle\\mathbf x,\\mathbf y\\rangle$ by reducing (summing) local scalar products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiixH1tCDOs9"
   },
   "outputs": [],
   "source": [
    "#\"to run\" syntax example: mpirun -n 10 python mpi.py 4000000\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "#read from command line\n",
    "N = int(sys.argv[1])    #length of vectors\n",
    "\n",
    "#arbitrary example vectors, generated to be evenly divided by the number of\n",
    "#processes for convenience\n",
    "\n",
    "x = np.random.rand(N) if comm.rank == 0 else None\n",
    "y = np.random.rand(N) if comm.rank == 0 else None\n",
    "\n",
    "#initialize as numpy arrays\n",
    "dot = np.array([0.])\n",
    "local_N = np.array([0])\n",
    "\n",
    "#test for conformability\n",
    "if rank == 0:\n",
    "                if (N != y.size):\n",
    "                                print (\"vector length mismatch\")\n",
    "                                comm.Abort()\n",
    "\n",
    "                #currently, our program cannot handle sizes that are not evenly divided by\n",
    "                #the number of processors\n",
    "                if (N % size != 0):\n",
    "                                print (\"the number of processors must evenly divide n.\")\n",
    "                                comm.Abort()\n",
    "\n",
    "                #length of each process's portion of the original vector\n",
    "                local_N = np.array([N//size])\n",
    "\n",
    "#communicate local array size to all processes\n",
    "comm.Bcast(local_N, root=0)\n",
    "\n",
    "#initialize as numpy arrays\n",
    "local_x = np.zeros(local_N)\n",
    "local_y = np.zeros(local_N)\n",
    "\n",
    "#divide up vectors\n",
    "comm.Scatter(x, local_x, root=0)\n",
    "comm.Scatter(y, local_y, root=0)\n",
    "\n",
    "#local computation of dot product\n",
    "local_dot = np.array([np.dot(local_x, local_y)])\n",
    "\n",
    "#sum the results of each\n",
    "comm.Reduce(local_dot, dot, op = MPI.SUM)\n",
    "\n",
    "if (rank == 0):\n",
    "                print (\"The dot product computed with MPI:\", dot[0])\n",
    "                print (\"The dot product computed w/o  MPI:\", np.dot(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjotqyGTHGfX",
    "outputId": "567cbe94-1410-45c6-91b5-469bb470a81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product computed with MPI: 999479.7318969145\n",
      "The dot product computed w/o  MPI: 999479.731896915\n"
     ]
    }
   ],
   "source": [
    "!mpirun --allow-run-as-root -n 10 python mpi.py 4000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcjQGyquDOtQ"
   },
   "source": [
    "\n",
    "**Exercise:** Why is the result $\\approx$ 1E6? A bad RNG?\n",
    "\n",
    "### Reduce-based computation of $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODozaK9rDOtS"
   },
   "outputs": [],
   "source": [
    "# run syntax example: mpirun -n 10 python mpi.py 4000000\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "#read from command line\n",
    "N = int(sys.argv[1])    #number of terms\n",
    "\n",
    "#initialize as numpy array\n",
    "s = np.array([0.])\n",
    "\n",
    "#test for conformability\n",
    "if (N % size != 0):\n",
    "    print (\"the number of processors must evenly divide n.\")\n",
    "    comm.Abort()\n",
    "\n",
    "#length of each process's portion of the original vector\n",
    "local_N = np.array([N/size])\n",
    "\n",
    "def getPartialSum(start, end):\n",
    "    a = np.arange(start, end)\n",
    "    return np.sum(1./(a*a))\n",
    "\n",
    "#local computation of partial sum\n",
    "local_s = getPartialSum(1+rank*local_N, 1+(rank+1)*local_N)\n",
    "local_s = np.array([local_s])\n",
    "\n",
    "#sum the results of each local sum\n",
    "comm.Reduce(local_s, s, op = MPI.SUM)\n",
    "\n",
    "if (rank == 0):\n",
    "    pi_approx = np.sqrt(6*s[0])\n",
    "    print (\"pi_approx:\", pi_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOuSPxKJDOtl"
   },
   "source": [
    "The program execution time can be measured with commands `time` or `/usr/bin/time -v`:\n",
    "\n",
    "(`real`: wall clock time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIc3lRQwJFjU",
    "outputId": "7e0b2d68-fc49-4a9e-efb4-ee44f0cfa20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_approx: 3.14159264404049\n",
      "\n",
      "real\t0m1.821s\n",
      "user\t0m0.609s\n",
      "sys\t0m1.138s\n",
      "pi_approx: 3.1415926440404958\n",
      "\n",
      "real\t0m1.325s\n",
      "user\t0m1.253s\n",
      "sys\t0m1.024s\n",
      "pi_approx: 3.1415926440404975\n",
      "\n",
      "real\t0m2.047s\n",
      "user\t0m2.176s\n",
      "sys\t0m1.493s\n"
     ]
    }
   ],
   "source": [
    "!time mpirun --allow-run-as-root -n 1 python mpi.py 100000000\n",
    "!time mpirun --allow-run-as-root -n 2 python mpi.py 100000000\n",
    "!time mpirun --allow-run-as-root -n 4 python mpi.py 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLSL8ssHKvq3"
   },
   "source": [
    "Speedup and efficiency of parallelization with 2 processes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbMya2FYDOto",
    "outputId": "604984ad-1a95-49de-b7d5-1956c868bea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup: 1.3743396226415094\n",
      "Efficiency: 0.6871698113207547\n"
     ]
    }
   ],
   "source": [
    "Speedup = 1.821/1.325\n",
    "print ('Speedup:', Speedup)\n",
    "\n",
    "Efficiency = Speedup/2\n",
    "print ('Efficiency:', Efficiency)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
